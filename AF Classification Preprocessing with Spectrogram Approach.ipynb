{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AF Classification Preprocessing with Spectrogram Approach\n",
    "The following notebook describes the code used to process ECG signals in order to be ready for training and evaluation from the multi - classificator built for this course project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and general variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wfdb\n",
    "from wfdb import processing \n",
    "from scipy.signal import medfilt\n",
    "import scipy\n",
    "import keras\n",
    "import pickle\n",
    "import os.path\n",
    "import QRS_util\n",
    "from tqdm import tqdm\n",
    "from random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "folderPath = \"./training2017/\"\n",
    "recordsPath = folderPath + 'REFERENCE.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading signals\n",
    "The preprocessing part starts by loading signals from Physionet Challenge and storing them, together with their labels, into data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadSignalsAndLabelsFromFile(folderPath):\n",
    "\n",
    "    signals = []\n",
    "    recordsFilePath = folderPath + 'REFERENCE.csv'\n",
    "    recordsAndLabels = pd.read_csv(recordsFilePath, header=None, names=['filename', 'label'])\n",
    "\n",
    "    if os.path.isfile(\"./RawSignals.pk1\") == False:\n",
    "\n",
    "        print('Getting raw signals ...')\n",
    "        for recordName in recordsAndLabels['filename']:     \n",
    "            recordName = folderPath + recordName\n",
    "            record = wfdb.rdrecord(recordName)\n",
    "            digitalSignal = record.adc()[:,0]\n",
    "            signals.append(digitalSignal)\n",
    "\n",
    "        print('Done.')\n",
    "        print('Saving signals to file ...')\n",
    "        with open ('./RawSignals.pk1', 'wb') as f:\n",
    "            pickle.dump(signals, f)\n",
    "    else:\n",
    "        print('Loading raw signals ...')\n",
    "\n",
    "        with open('./RawSignals.pk1', 'rb') as fp:\n",
    "            signals = pickle.load(fp)\n",
    "            print('Done.') \n",
    "\n",
    "    signals = np.array(signals)\n",
    "    y = recordsAndLabels['label']\n",
    "\n",
    "    return signals, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into training and test set\n",
    "Now our dataset is created, we are going to divide it into a training and test set. We splitted it with an 80 / 20 approach and with a stratify approach, which made us able to mantain this percentage split for each class of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplit(signals, labels):\n",
    "\n",
    "    if len(signals) and len(labels)> 0:\n",
    "        df = {\n",
    "\n",
    "            'signal' : signals,\n",
    "            'label' : labels\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(df, columns = ['signal', 'label'])\n",
    "\n",
    "        # Keep 20% of the data out for validation\n",
    "        train_reference_df, val_reference_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=123)\n",
    "\n",
    "        # 'N' = 0\n",
    "        # 'A' = 1\n",
    "        # 'O' = 2\n",
    "        # '~' = 3\n",
    "\n",
    "        return train_reference_df, val_reference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline wander removal using Butterworth filter\n",
    "In order to remove baseline wander from ECGs, we passed them through a Butterworth filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaselineWanderFilter(signals):\n",
    "\n",
    "    print('Filtering ...')\n",
    "    # Sampling frequency\n",
    "    fs = 300\n",
    "\n",
    "    for i, signal in enumerate(signals):\n",
    "\n",
    "        # Baseline estimation\n",
    "        win_size = int(np.round(0.2 * fs)) + 1\n",
    "        baseline = medfilt(signal, win_size)\n",
    "        win_size = int(np.round(0.6 * fs)) + 1\n",
    "        baseline = medfilt(baseline, win_size)\n",
    "\n",
    "        # Removing baseline\n",
    "        filt_data = signal - baseline\n",
    "        signals[i] = filt_data\n",
    "    \n",
    "    print('Storing filtered signals..')\n",
    "    with open ('./FilteredSignals.pk1', 'wb') as f:\n",
    "        pickle.dump((signals), f)\n",
    "    print('Done.')\n",
    "\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing data augmentation on training set\n",
    "In order to increment the number of the observations for the test set, we doubled up the number of signals for AFib and Noise class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateNoiseVector():\n",
    "    \n",
    "    noise = np.random.normal(0, 2000, size = (9000,))\n",
    "    noise = noise.astype(np.int64)\n",
    "    return noise\n",
    "\n",
    "def TrainingTestAugumentationSpectrogram(trainingSet):\n",
    "\n",
    "    y = trainingSet['label']\n",
    "    signals = trainingSet['signal']\n",
    "    signals = signals.to_numpy()\n",
    "\n",
    "    print('Class distribution before doubling up ...')\n",
    "    print('Normal: ', len(np.where(y == 'N')[0]))\n",
    "    print('AF: ', len(np.where(y == 'A')[0]))\n",
    "    print('Other: ', len(np.where(y == 'O')[0]))\n",
    "    print('Noisy: ', len(np.where(y == '~')[0]))\n",
    "\n",
    "    # Double up AF signals\n",
    "\n",
    "    aFibMask = np.where(y == 'A')\n",
    "    aFibSignals = np.array(signals[aFibMask])\n",
    "\n",
    "    signals = np.hstack((signals, aFibSignals))\n",
    "    newAfibLabels = ['A']*len(aFibSignals)\n",
    "    y = np.append(y, newAfibLabels)\n",
    "\n",
    "    print('New AF observations number: ', len(np.where(y == 'A')[0]))\n",
    "\n",
    "    # Duplicating noisy signals to add\n",
    "\n",
    "    noiseMask = np.where(y == '~')\n",
    "    noiseSignals = np.array(signals[noiseMask])\n",
    "\n",
    "    for i, signal in enumerate(noiseSignals):\n",
    "        noiseSignals[i] = CreateNoiseVector()\n",
    "\n",
    "    signals = np.hstack((signals, noiseSignals))\n",
    "    #   signals = np.hstack((signals, noiseSignals))\n",
    "    newNoiseLabels = ['~']*len(noiseSignals)\n",
    "    y = np.append(y, newNoiseLabels)\n",
    "\n",
    "    print('New Noise observations number: ', len(np.where(y == '~')[0]))\n",
    "       \n",
    "    print('Done.')\n",
    "\n",
    "    dataset = {\n",
    "\n",
    "        'signal' : signals,\n",
    "        'label' : y\n",
    "    }\n",
    "\n",
    "    dataset = pd.DataFrame(dataset, columns = ['signal', 'label'])\n",
    "\n",
    "    print('Storing signals and labels to file..')\n",
    "    with open ('./TrainingTestAugmented.pk1', 'wb') as f:\n",
    "        pickle.dump((dataset), f)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping signals into a fixed length\n",
    "In order to create a machine-friendly format, all the ECGs were cropped up into a fixed length of 9000. The approach used for cropping depends on the nature of the dataset (training or test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomCrop(df, target_size=9000, center_crop=True):\n",
    "    \n",
    "    signals = df['signal']\n",
    "    labels = df['label']\n",
    "    newSignals = []\n",
    "    print('Cropping data ...')\n",
    "\n",
    "    for i, data in enumerate(signals):\n",
    "\n",
    "        N = data.shape[0]\n",
    "        \n",
    "        # Return data if correct size\n",
    "        if N == target_size:\n",
    "            newSignals.append(data)\n",
    "            continue\n",
    "        \n",
    "        # If data is too small, then pad with zeros\n",
    "        if N < target_size:\n",
    "            tot_pads = target_size - N\n",
    "            left_pads = int(np.ceil(tot_pads / 2))\n",
    "            right_pads = int(np.floor(tot_pads / 2))\n",
    "            newSignal = np.pad(data, [left_pads, right_pads], mode='constant')\n",
    "            newSignals.append(newSignal)\n",
    "            continue\n",
    "\n",
    "        # Random Crop (always centered if center_crop=True)\n",
    "        if center_crop:\n",
    "            from_ = int((N / 2) - (target_size / 2))\n",
    "        else:\n",
    "            from_ = np.random.randint(0, np.floor(N - target_size))\n",
    "        to_ = from_ + target_size\n",
    "        newSignal = data[from_:to_]\n",
    "        newSignals.append(newSignal)\n",
    "    \n",
    "    dataset = {\n",
    "\n",
    "        'signal' : newSignals,\n",
    "        'label' : labels\n",
    "    }\n",
    "\n",
    "    dataset = pd.DataFrame(dataset, columns = ['signal', 'label'])\n",
    "\n",
    "    print('Done.')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing FFT and its logarithm\n",
    "To use an ECG signal as an image, we had to use Fast Fourier Transform (FFT) in order to obtain a 2D spectrogram. Logarithm is then used to normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSpectrogramFromSignal(signal):\n",
    "    _, _, sg = scipy.signal.spectrogram(signal, fs=300, window=('tukey', 0.25), \n",
    "            nperseg=64, noverlap=0.5, return_onesided=True)\n",
    "\n",
    "    return sg.T\n",
    "\n",
    "def FFT(dataset):\n",
    "\n",
    "    signals = dataset['signal']\n",
    "    labels = dataset['label']\n",
    "\n",
    "    print('Computing FFTs ...')\n",
    "    logSpectrograms = []\n",
    "\n",
    "    for s in signals:\n",
    "\n",
    "        logSp = np.log(GenerateSpectrogramFromSignal(s) + 1)\n",
    "        if logSp.shape[0] != 140:\n",
    "            print('Shape diverso da 140')\n",
    "            continue\n",
    "        logSpectrograms.append(logSp)\n",
    "\n",
    "    logSpectrograms = np.array(logSpectrograms)\n",
    "    means = logSpectrograms.mean(axis=(1,2))\n",
    "    stds = logSpectrograms.std(axis=(1,2))\n",
    "    logSpectrograms = np.array([(log - mean) / std for log, mean, std in zip(logSpectrograms, means, stds)])\n",
    "    logSignals = logSpectrograms[..., np.newaxis]\n",
    "    \n",
    "    print('Storing log signals to file..')\n",
    "    with open ('./LogSignals.pk1', 'wb') as f:\n",
    "        pickle.dump(logSignals, f)\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return logSignals, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data\n",
    "The last preprocessing step consists on normalizing data by taking the signals belonging to the 5% and 99% of the total. This becomes our normalization factor, and each signal value is divided by that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(signals, labels):\n",
    "\n",
    "    print('Normalizing data ...')\n",
    "    for i, signal in enumerate(signals):\n",
    "\n",
    "        # Amplitude estimate\n",
    "        norm_factor = np.percentile(signal, 99) - np.percentile(signal, 5)\n",
    "        signals[i] = signal / norm_factor\n",
    "\n",
    "    print('Done.')\n",
    "    return signals, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing wrapping up method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessingForSpectrogramApproach():\n",
    "    signals, labels = LoadSignalsAndLabelsFromFile(folderPath)  \n",
    "\n",
    "    if os.path.isfile('./FilteredSignals.pk1'):\n",
    "        print('Loading previously filtered signals ...')\n",
    "        with open('./FilteredSignals.pk1', 'rb') as fp:\n",
    "            signals = pickle.load(fp)\n",
    "    else:\n",
    "        signals = BaselineWanderFilter(signals)\n",
    "\n",
    "    trainingSet, testSet = TrainTestSplit(signals, labels)\n",
    "    trainingSet = TrainingTestAugumentationSpectrogram(trainingSet)\n",
    "    trainingSet = RandomCrop(trainingSet, center_crop = False)\n",
    "    trainSignals, labels = FFT(trainingSet)\n",
    "    trainSignals, traininglabels = NormalizeData(trainSignals, labels) \n",
    "    \n",
    "    testSet = RandomCrop(testSet, center_crop = True)\n",
    "    testSignals, labels = FFT(testSet)\n",
    "    testSignals, testLabels = NormalizeData(testSignals, labels) \n",
    "\n",
    "    print('Storing training set to file..')\n",
    "    with open('./TrainingSetFFT.pk1', 'wb') as f:\n",
    "        print('Len  train:' + str(len(trainSignals)))\n",
    "        pickle.dump((trainSignals, traininglabels), f)\n",
    "            \n",
    "    print('Storing test set to file..')\n",
    "    with open ('./TestSetFFT.pk1', 'wb') as f:\n",
    "        print('Len  test:' + str(len(testSignals)))\n",
    "        pickle.dump((testSignals, testLabels), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting raw signals ...\n",
      "Done.\n",
      "Saving signals to file ...\n",
      "Filtering ...\n"
     ]
    }
   ],
   "source": [
    "PreprocessingForSpectrogramApproach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
